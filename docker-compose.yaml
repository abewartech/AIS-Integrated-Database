version: '3.2'
services:
  # ---------------------------------------------
  # PROCESSING CHAIN: to do the actual work
  # ---------------------------------------------
  file_reader:
    # Reads a folder full of daily AIS logs, decodes them and feeds them onto a rabbitmq topic 
      build:
        context: ./build/file_streamer
        dockerfile: Dockerfile
      hostname: file_reader
      env_file:
        - .env  #This both the docker-compose env file (to set ports names, image tags etc) and the internal container config (handles passwords, ports to connect to etc.)
      volumes:
        #  - /etc/localtime:/etc/localtime:ro #This is useful in Linux, doesn't work on Windows. 
        - ./data/:${CONTAINER_FILE_DIR}
      command: python /usr/local/file_streamer/main.py  -ll 'INFO' 
  
  rabbit:
  # Wonderful. Perfect. Amazing. 
    container_name: rabbit
    hostname: ${RABBIT_HOST}
    image: rabbitmq:${RABBIT_TAG}
    ports:
      - ${RABBIT_MSG_PORT}:5672
      - ${RABBIT_MANAGE_PORT}:15672
    environment: 
        RABBITMQ_DEFAULT_USER: ${RABBITMQ_DEFAULT_USER}
        RABBITMQ_DEFAULT_PASS: ${RABBITMQ_DEFAULT_PASS}
    # volumes:
      # - ./volumes/rabbitmq/data/:/var/lib/rabbitmq/
      # - ./volumes/rabbitmq/etc/:/etc/rabbitmq/
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: 10m 

  db_inserter: 
  # Builds a queue that's connected to the file_source output. Packages the messages into a DB acceptable format and 
  # sends them to an async worker to do the actual insert. It's much faster to do bulk inserts than it is to try
  # insert every single message alone.  
    # Reads a folder full of daily AIS logs, decodes them and feeds them onto a rabbitmq topic 
    build:
      context: ./build/db_inserter
      dockerfile: Dockerfile
    hostname: db_inserter
    env_file:
      - .env  #This both the docker-compose env file (to set ports names, image tags etc) and the internal container config (handles passwords, ports to connect to etc.)
    # volumes:
      #  - /etc/localtime:/etc/localtime:ro #This is useful in Linux, doesn't work on Windows. 
    command: python /usr/local/db_inserter/main.py -ll 'INFO' 
        
  db:
  # The database that holds all the AIS data and maybe some other nice to have's like:
  #   - World Port Index'
  #   - World EEZ shapefile
  # The schema is also set up in the start up scripts. 
  # https://hub.docker.com/r/timescale/timescaledb-postgis/tags
    container_name: db
    hostname: ${DB_HOST}
    image: timescale/timescaledb-postgis:${TIMESCALE_TAG}
    command: postgres -c shared_preload_libraries=timescaledb
    ports:
      - ${DB_PORT}:5432
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_UID=${UID}
      - POSTGRES_GID=${GID}
    volumes:
      - ./volumes/db/:/var/lib/postgresql/data
      - ./build/db/db_init:/docker-entrypoint-initdb.d/ 
      - ./build/db/db_init_data/:/tmp
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: 10m
  
  postgrest:
      # Postgrest container. Provides API to DB
    # https://postgrest.org/en/stable/install.html 
    container_name: ${PROJECT_NAME}_postgrest
    hostname: ${POSTGREST_HOST}
    # image: postgrest/postgrest:${POSTGREST_TAG} 
    build:
      context: ./build/postgrest
      dockerfile: Dockerfile
      args:
        - tag=${POSTGREST_TAG}
    ports:
      - ${POSTGREST_PORT}:3000
    environment:
      PGRST_DB_URI=postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${DB_HOST}:${DB_INTERNAL_PORT}/${POSTGRES_DB}
      PGRST_DB_SCHEMA=${PGRST_DB_SCHEMA}
      PGRST_DB_ANON_ROLE=${PGRST_DB_ANON_ROLE} #In production this role should not be the same as the one used for the connection

    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: 10m

      - db

  # ---------------------------------------------
  # USER TOOLS: to make your life easier
  # ---------------------------------------------
  pgadmin:
    container_name: pgadmin_phd
    # Method to interrogate the DB
    image: dpage/pgadmin4:${PGADMIN_TAG}
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_DEFAULT_EMAIL}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_DEFAULT_PASSWORD}
    volumes:
        - ./volumes/pgadmin/:/var/lib/pgadmin
    ports:
      - ${PGADMIN_PORT}:80
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: 10m
 
  # jup-notebook:
  #   # Has all the libs installed for datascience. Make sure to save your work under joyvan/work/<some dir> to have
  #   # it persist between shutdowns
  #   image: jupyter/datascience-notebook:${JUP_TAG}
  #   volumes:
  #     - ./volumes/jupyter:/home/jovyan/work
  #   ports:
  #     - ${JUP_PORT}:8888
  #     - 8787:8787
  #   container_name:   jup-notebook
  #   restart: unless-stopped
  #   logging:
  #     driver: json-file
  #     options:
  #       max-size: 10m

  # geoserver:
  #   image: kartoza/geoserver:${GEOSERVER_TAG} 
  #   volumes:
  #     - ./volumes/geoserver/:/opt/geoserver/data_dir
  #   ports:
  #     - ${GEOSERVER_PORT}:8080
  #   restart: on-failure
    # logging:
    #   driver: json-file
    #   options:
    #     max-size: 10m
 